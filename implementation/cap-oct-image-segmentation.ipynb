{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip3 install -r requirements.txt"]},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-09-12T17:15:44.759122Z","iopub.status.busy":"2024-09-12T17:15:44.758495Z","iopub.status.idle":"2024-09-12T17:15:44.769603Z","shell.execute_reply":"2024-09-12T17:15:44.768651Z","shell.execute_reply.started":"2024-09-12T17:15:44.759062Z"},"trusted":true},"outputs":[],"source":["# IMPORTS\n","\n","import os\n","import sys\n","import glob\n","import cv2\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch\n","from torch import nn, optim\n","from torch.utils.data import DataLoader\n","import torch.nn.functional as F\n","\n","import pytorch_lightning as pl\n","from pytorch_lightning.loggers import TensorBoardLogger\n","\n","from torchmetrics.classification import MulticlassJaccardIndex, MulticlassAccuracy\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-09-12T17:15:44.772309Z","iopub.status.busy":"2024-09-12T17:15:44.771598Z","iopub.status.idle":"2024-09-12T17:15:44.781067Z","shell.execute_reply":"2024-09-12T17:15:44.780107Z","shell.execute_reply.started":"2024-09-12T17:15:44.772276Z"},"trusted":true},"outputs":[],"source":["# VARIABLES\n","\n","# Parameters\n","SIZE_X = 512    # Image height\n","SIZE_Y = 256    # Image width\n","n_classes = 14  # Number of classes for segmentation\n","\n","# Paths\n","TRAIN_PATH_X = './data/train/images'\n","TRAIN_PATH_Y = './data/train/masks'\n","TEST_PATH_X = './data/test/images'\n","TEST_PATH_Y = './data/test/masks'\n","\n","# Training parameters\n","num_epochs = 250\n","batch_size = 16 # Higher is better but requires more memory\n","\n","# Set print options to display full arrays\n","np.set_printoptions(threshold=sys.maxsize)\n","\n","# Define device (GPU or CPU)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# TensorBoard logger\n","logger = TensorBoardLogger('tb_logs', name='1')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-12T17:15:44.783057Z","iopub.status.busy":"2024-09-12T17:15:44.782674Z","iopub.status.idle":"2024-09-12T17:15:49.536463Z","shell.execute_reply":"2024-09-12T17:15:49.534304Z","shell.execute_reply.started":"2024-09-12T17:15:44.783023Z"},"trusted":true},"outputs":[],"source":["# IMAGE LOADING AND PROCESSING\n","\n","# Get image and mask file paths\n","train_ids_x = next(os.walk(TRAIN_PATH_X))[2]\n","train_ids_y = next(os.walk(TRAIN_PATH_Y))[2]\n","test_ids_x = next(os.walk(TEST_PATH_X))[2]\n","test_ids_y = next(os.walk(TEST_PATH_Y))[2]\n","\n","#  Get and resize images\n","def get_resize_image(path):\n","    images = []\n","    for img_path in glob.glob(os.path.join(path, \"*.png\")): # Change the extension if needed\n","        img = cv2.imread(img_path, 0)                       # Read the image in grayscale mode    \n","        img = cv2.resize(img, (SIZE_Y, SIZE_X))             # Resize the image\n","        images.append(img)                                  # Append the image to the list\n","    return images                                           # Convert the list to a NumPy array\n","\n","# Get and resize masks\n","def get_resize_masks(path):\n","    masks = []\n","    for mask_path in glob.glob(os.path.join(path, \"*.png\")):\n","        mask = cv2.imread(mask_path, 0)\n","        mask = cv2.resize(mask, (SIZE_Y, SIZE_X), interpolation=cv2.INTER_NEAREST)  # Prevent interpolation for the masks (nearest neighbor)\n","        masks.append(mask)\n","    return masks\n","\n","train_images = np.array(get_resize_image(TRAIN_PATH_X)) # Convert the list to a NumPy array\n","train_masks = np.array(get_resize_masks(TRAIN_PATH_Y))\n","\n","test_images = get_resize_image(TEST_PATH_X) # No need to convert to NumPy array, only for testing\n","test_masks = np.array(get_resize_masks(TEST_PATH_Y))\n","\n","# Encode labels between 0 and n_classes, because PyTorch expects labels to be in this format for segmentation tasks \n","def encode(labels):\n","    label_encoder = LabelEncoder()                                             # Initialize the label encoder\n","    n, h, w = labels.shape                                                     # Get the shape of the masks\n","    labels_reshaped = labels.reshape(-1, 1)                                    # Reshape the masks to 2 dimensions\n","    labels_reshaped_encoded = label_encoder.fit_transform(labels_reshaped)     # Fit and transform the labels to fit between 0 and n_classes\n","    labels_encoded_original_shape = labels_reshaped_encoded.reshape(n, h, w)   # Reshape the encoded labels back to the original shape\n","    return labels_encoded_original_shape\n","\n","train_masks_encoded = encode(train_masks)\n","test_masks_encoded = encode(test_masks)\n","\n","# Expand dimensions to add a channel dimension for images and masks and normalize the images between 0 and 1\n","train_images = np.expand_dims(train_images, axis=3)\n","train_images = train_images / 255.0  # Normalization\n","\n","test_images = np.expand_dims(test_images, axis=3)\n","test_images = test_images / 255.0  # Normalization\n","\n","train_masks_input = np.expand_dims(train_masks_encoded, axis=3)\n","test_masks_input = np.expand_dims(test_masks_encoded, axis=3)\n","\n","# Split the data into training and testing sets (80% training, 20% testing)\n","x_train, X_test, y_train, y_test = train_test_split(train_images, train_masks_input, test_size=0.20, random_state=0)\n","\n","# Convert labels to one-hot encoded format to match the output of the model (n_classes)\n","def one_hot_encode(y, num_classes):\n","    return np.eye(num_classes)[y.reshape(-1)].reshape((*y.shape[:-1], num_classes)) # eye creates an identity matrix of shape (num_classes, num_classes) and then reshapes it to the shape \n","                                                                                    # of y with num_classes columns to get one-hot encoded labels\n","                                                                                    # *y.shape[:-1] is (256, 256) and y.reshape(-1) reshapes it to a 1D array\n","                                                                                    \n","train_masks_one_hot = one_hot_encode(y_train, num_classes=n_classes)\n","y_train_one_hot = train_masks_one_hot.reshape((y_train.shape[0], y_train.shape[1], y_train.shape[2], n_classes)) # Reshape the one-hot encoded labels to the original shape\n","\n","test_masks_one_hot = one_hot_encode(y_test, num_classes=n_classes)\n","y_test_one_hot = test_masks_one_hot.reshape((y_test.shape[0], y_test.shape[1], y_test.shape[2], n_classes)) # Reshape the one-hot encoded labels to the original shape\n","\n","# Convert the data to PyTorch tensors, permute the dimensions to match the format (N, C, H, W), and move them to the device\n","x_train = torch.tensor(x_train, dtype=torch.float32).permute(0, 3, 1, 2).to(device) \n","y_train_one_hot = torch.tensor(y_train_one_hot, dtype=torch.float32).permute(0, 3, 1, 2).to(device)\n","X_test = torch.tensor(X_test, dtype=torch.float32).permute(0, 3, 1, 2).to(device)\n","y_test_one_hot = torch.tensor(y_test_one_hot, dtype=torch.float32).permute(0, 3, 1, 2).to(device)\n","test_images = torch.tensor(test_images, dtype=torch.float32).permute(0, 3, 1, 2).to(device)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.status.busy":"2024-09-12T17:15:49.538004Z","iopub.status.idle":"2024-09-12T17:15:49.538574Z","shell.execute_reply":"2024-09-12T17:15:49.538318Z","shell.execute_reply.started":"2024-09-12T17:15:49.538291Z"},"trusted":true},"outputs":[],"source":["# UNET MODEL\n","\n","class UNet(pl.LightningModule):\n","    def __init__(self, n_classes, img_channels):\n","        super(UNet, self).__init__()\n","\n","        # Contraction path (encoder)\n","        # It takes the input image and reduces its spatial dimensions while increasing the number of channels.\n","        # This is done by applying a series of convolutions and pooling layers.\n","        # The output of each convolutional layer is passed through a ReLU activation function, which introduces non-linearity.\n","        # The output of each pooling layer is passed to the next layer and the output of the last layer is the input to the expansive path.\n","        \n","        self.conv1 = nn.Conv2d(img_channels, 16, kernel_size=3, padding=1) # The first convolutional layer takes the input image and applies 16 filters of size 3x3.\n","        self.conv1_2 = nn.Conv2d(16, 16, kernel_size=3, padding=1)         # The second convolutional layer takes the output of the first layer and applies 16 filters of size 3x3.\n","        self.dropout1 = nn.Dropout(0.1)                                    # The dropout layer is used to prevent overfitting by randomly setting a fraction of the input units to zero.\n","        self.pool1 = nn.MaxPool2d(2, 2)                                    # The pooling layer reduces the spatial dimensions of the input by taking the maximum value in each 2x2 region.\n","\n","        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)           # The third convolutional layer takes the output of the pooling layer and applies 32 filters of size 3x3 and so on.\n","        self.conv2_2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n","        self.dropout2 = nn.Dropout(0.1)\n","        self.pool2 = nn.MaxPool2d(2, 2)\n","\n","        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n","        self.conv3_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n","        self.dropout3 = nn.Dropout(0.2)\n","        self.pool3 = nn.MaxPool2d(2, 2)\n","\n","        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n","        self.conv4_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n","        self.dropout4 = nn.Dropout(0.2)\n","        self.pool4 = nn.MaxPool2d(2, 2)\n","\n","        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n","        self.conv5_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n","        self.dropout5 = nn.Dropout(0.3)\n","\n","        # Expansive path (decoder)\n","        # It takes the output of the last convolutional layer in the contraction path and increases the spatial dimensions while reducing the number of channels.\n","        # This is done by applying a series of transposed convolutions and concatenating the output of each transposed convolution with the output of the corresponding convolution in the contraction path.\n","        # The output of each transposed convolution is passed through a ReLU activation function, which introduces non-linearity.\n","        # The output of the last transposed convolution is the final output of the network.\n","\n","        self.up6 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2) # The first transposed convolutional layer takes the output of the last convolutional layer in the contraction path and applies 128 filters of size 2x2.\n","        self.conv6 = nn.Conv2d(256, 128, kernel_size=3, padding=1)       # The second convolutional layer takes the output of the transposed convolutional layer and applies 128 filters of size 3x3.\n","        self.conv6_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)     # The third convolutional layer takes the output of the second convolutional layer and applies 128 filters of size 3x3.\n","        self.dropout6 = nn.Dropout(0.2)                                  # The dropout layer is used to prevent overfitting by randomly setting a fraction of the input units to zero.\n","\n","        self.up7 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n","        self.conv7 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n","        self.conv7_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n","        self.dropout7 = nn.Dropout(0.2)\n","\n","        self.up8 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n","        self.conv8 = nn.Conv2d(64, 32, kernel_size=3, padding=1)\n","        self.conv8_2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n","        self.dropout8 = nn.Dropout(0.1)\n","\n","        self.up9 = nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2)\n","        self.conv9 = nn.Conv2d(32, 16, kernel_size=3, padding=1)\n","        self.conv9_2 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n","        self.dropout9 = nn.Dropout(0.1)\n","\n","        self.conv10 = nn.Conv2d(16, n_classes, kernel_size=1)\n","\n","    # Computations performed at each forward pass of the network.\n","    def forward(self, x):\n","        # Contraction path\n","        c1 = F.relu(self.conv1(x))      # The input image is passed through the first convolutional layer and the output is passed through a ReLU activation function.\n","        c1 = F.relu(self.conv1_2(c1))   # The output of the first convolutional layer is passed through the second convolutional layer and the output is passed through a ReLU activation function.\n","        c1 = self.dropout1(c1)          # The output of the second convolutional layer is passed through a dropout layer.\n","        p1 = self.pool1(c1)             # The output of the dropout layer is passed through a pooling layer.\n","\n","        c2 = F.relu(self.conv2(p1))\n","        c2 = F.relu(self.conv2_2(c2))\n","        c2 = self.dropout2(c2)\n","        p2 = self.pool2(c2)\n","\n","        c3 = F.relu(self.conv3(p2))\n","        c3 = F.relu(self.conv3_2(c3))\n","        c3 = self.dropout3(c3)\n","        p3 = self.pool3(c3)\n","\n","        c4 = F.relu(self.conv4(p3))\n","        c4 = F.relu(self.conv4_2(c4))\n","        c4 = self.dropout4(c4)\n","        p4 = self.pool4(c4)\n","\n","        c5 = F.relu(self.conv5(p4))\n","        c5 = F.relu(self.conv5_2(c5))\n","        c5 = self.dropout5(c5)\n","\n","        # Expansive path\n","        u6 = self.up6(c5)               # The output of the last convolutional layer in the contraction path is passed through the first transposed convolutional layer.\n","        u6 = torch.cat([u6, c4], dim=1) # The output of the transposed convolutional layer is concatenated with the output of the corresponding convolution in the contraction path.\n","        c6 = F.relu(self.conv6(u6))     # The concatenated output is passed through the second convolutional layer and the output is passed through a ReLU activation function.\n","        c6 = F.relu(self.conv6_2(c6))   # The output of the second convolutional layer is passed through the third convolutional layer and the output is passed through a ReLU activation function.\n","        c6 = self.dropout6(c6)          # The output of the third convolutional layer is passed through a dropout layer.\n","\n","        u7 = self.up7(c6)\n","        u7 = torch.cat([u7, c3], dim=1)\n","        c7 = F.relu(self.conv7(u7))\n","        c7 = F.relu(self.conv7_2(c7))\n","        c7 = self.dropout7(c7)\n","\n","        u8 = self.up8(c7)\n","        u8 = torch.cat([u8, c2], dim=1)\n","        c8 = F.relu(self.conv8(u8))\n","        c8 = F.relu(self.conv8_2(c8))\n","        c8 = self.dropout8(c8)\n","\n","        u9 = self.up9(c8)\n","        u9 = torch.cat([u9, c1], dim=1)\n","        c9 = F.relu(self.conv9(u9))\n","        c9 = F.relu(self.conv9_2(c9))\n","        c9 = self.dropout9(c9)\n","\n","        outputs = self.conv10(c9)      # The output of the last convolutional layer is passed through a convolutional layer with a kernel size of 1x1 to produce the final output of the network.\n","\n","        return outputs\n","\n","    # The training step is used to compute the training loss and update the weights of the network.\n","    def training_step(self, batch, batch_idx):\n","        x, y = batch                     # The input image and the target mask are unpacked from the batch.\n","        y_hat = self(x)                  # The input image is passed through the network to produce the output mask.\n","        y = y.squeeze(-1)                # The target mask is reshaped to match the output mask.\n","        loss = F.cross_entropy(y_hat, y) # The cross-entropy loss is computed between the output mask and the target mask.\n","        self.log(\"train_loss\", loss)     # The loss is logged for visualization.\n","        return loss\n","    \n","    # The validation step is used to compute the validation loss.\n","    def validation_step(self, batch, batch_idx):\n","        x,y = batch                           # The input image and the target mask are unpacked from the batch.\n","        y_hat = self(x)                       # The input image is passed through the network to produce the output mask.\n","        y = y.squeeze(-1)                     # The target mask is reshaped to match the output mask.\n","        val_loss = F.cross_entropy(y_hat, y)  # The cross-entropy loss is computed between the output mask and the target mask.\n","        self.log(\"val_loss\", val_loss)        # The loss is logged for visualization.\n","        return val_loss                         \n","    \n","    def configure_optimizers(self):\n","        return torch.optim.Adam(self.parameters(), lr=1e-3)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-09-12T17:15:49.540036Z","iopub.status.idle":"2024-09-12T17:15:49.540600Z","shell.execute_reply":"2024-09-12T17:15:49.540330Z","shell.execute_reply.started":"2024-09-12T17:15:49.540303Z"},"trusted":true},"outputs":[],"source":["# TRAINING\n","\n","# Initialize the model and move it to the device\n","model = UNet(n_classes=n_classes, img_channels=1).to(device) \n","\n","# Define optimizer and loss function\n","optimizer = optim.Adam(model.parameters(), lr=1e-4)\n","criterion = nn.CrossEntropyLoss() # CrossEntropyLoss for multi-class segmentation\n","\n","train_loader = DataLoader(list(zip(x_train, y_train_one_hot)), batch_size=batch_size, shuffle=True) # DataLoader for training, shuffle the data for better training and generalization\n","test_loader = DataLoader(list(zip(X_test, y_test_one_hot)), batch_size=batch_size, shuffle=False)   # DataLoader for testing\n","\n","train_losses, val_losses = [], [] # Lists to store training and validation losses\n","\n","trainer = pl.Trainer(max_epochs=num_epochs, logger=logger) # PyTorch Lightning Trainer\n","\n","trainer.fit(model, train_loader, test_loader) # Fit the model\n","    \n","# Save the model\n","torch.save(model.state_dict(), 'model.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-09-12T17:15:49.544212Z","iopub.status.idle":"2024-09-12T17:15:49.544689Z","shell.execute_reply":"2024-09-12T17:15:49.544473Z","shell.execute_reply.started":"2024-09-12T17:15:49.544448Z"},"trusted":true},"outputs":[],"source":["# MEAN IoU\n","\n","# Put the model in evaluation mode and calculate the mean IoU (Jaccard Index) and Accuracy on the test set\n","model.eval().to(device)\n","jaccard_idx = MulticlassJaccardIndex(num_classes=n_classes).to(device)\n","accuracy = MulticlassAccuracy(num_classes=n_classes).to(device)\n","\n","y_pred = []\n","y_true = []\n","\n","# Get predictions and true labels for the test set and calculate the mean IoU\n","with torch.no_grad():\n","    for inputs, labels in test_loader:\n","        outputs = model(inputs)                     # Get the predictions\n","        preds = torch.argmax(outputs, dim=1)        # Get the predicted class by finding the index with the maximum value in the class dimension\n","        y_pred.append(preds)                        # Append the predictions to the list\n","        y_true.append(torch.argmax(labels, dim=1))  # Get the true class from the one-hot encoded labels\n","\n","# Concatenate the lists into tensors\n","y_pred = torch.cat(y_pred, dim=0)\n","y_true = torch.cat(y_true, dim=0)\n","\n","# Calculate and print the IoU (Jaccard Index) & Accuracy\n","iou = jaccard_idx(y_pred, y_true)\n","acc = accuracy(y_pred, y_true)\n","print(\"Mean IoU =\", iou.item())\n","print(\"Accuracy =\", acc.item())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-12T17:42:02.492458Z","iopub.status.busy":"2024-09-12T17:42:02.491765Z","iopub.status.idle":"2024-09-12T17:42:03.520558Z","shell.execute_reply":"2024-09-12T17:42:03.519493Z","shell.execute_reply.started":"2024-09-12T17:42:02.492416Z"},"trusted":true},"outputs":[],"source":["# TESTING\n","\n","# TODO: Maybe add a loop to get multiple test images\n","test_img = test_images[0]\n","\n","# Save a copy of the test image for plotting and convert it to a NumPy array with the correct shape\n","test_img_cpy = test_img.clone().detach().cpu().numpy()\n","test_img_cpy = np.squeeze(test_img_cpy, axis=0)         # Remove the batch dimension\n","test_img_cpy = np.expand_dims(test_img_cpy, axis=2)     # Add the channel dimension\n","\n","# Get the first test mask (ground truth)\n","ground_truth = test_masks[0]\n","\n","# Normalize the test image to be between 0 and 1 and add a channel dimension\n","test_img_norm = test_img[:, :, 0][:, :, None] \n","\n","# Ensure the image is on the correct device and shape\n","test_img_input = test_img.clone().detach().unsqueeze(0).to(device)\n","\n","# Get prediction from the model\n","prediction = model(test_img_input)\n","\n","# Get the predicted mask by finding the index with the maximum value in the class dimension\n","# and convert it to a NumPy array\n","predicted_img = torch.argmax(prediction, dim=1).squeeze(0).cpu().numpy()\n","\n","# Test image has to be moved to the CPU and converted to numpy for plotting\n","test_img = test_img.cpu().numpy()\n","\n","# Color map for the masks with the RGB values used in DeepVision\n","color_map = np.array([\n","        [0, 0, 0],       # Background\n","        [229, 4, 2],     # ILM\n","        [49, 141, 171],  # RNFL\n","        [138, 61, 199],  # GCL\n","        [154, 195, 239], # IPL\n","        [245, 160, 56],  # INL\n","        [232, 146, 141], # OPL\n","        [245, 237, 105], # ONL\n","        [232, 206, 208], # ELM\n","        [128, 161, 54],  # PR\n","        [32, 207, 255],  # RPE\n","        [232, 71, 72],   # BM\n","        [212, 182, 222], # CC\n","        [196, 45, 4],    # CS\n","])\n","\n","# Apply the color map to the predicted and ground truth masks\n","predicted_img_color = color_map[predicted_img]\n","ground_truth_color = color_map[ground_truth]\n","\n","# Plot the predicted blended image\n","plt.figure(figsize=(10, 5))\n","alpha = 0.5\n","plt.subplot(111)\n","plt.title('Blended Prediction')\n","plt.imshow(test_img_cpy[:, :, 0], cmap='gray')\n","plt.imshow(predicted_img_color, cmap='jet', alpha=alpha)\n","\n","# Save the blended plot to a file\n","plt.savefig('prediction.png')\n","\n","plt.show()\n","\n","# Plot the three seperate images\n","plt.figure(figsize=(20, 10))\n","\n","# Test image\n","plt.subplot(231)\n","plt.title('Testing Image')\n","plt.imshow(test_img_cpy[:, :, 0], cmap='gray')\n","\n","# Ground truth mask\n","plt.subplot(232)\n","plt.title('Ground Truth')\n","plt.imshow(ground_truth_color, cmap='jet')\n","\n","# Predicted mask\n","plt.subplot(233)\n","plt.title('Model Prediction')\n","plt.imshow(predicted_img_color, cmap='jet')\n","plt.show()"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5675963,"sourceId":9361469,"sourceType":"datasetVersion"}],"dockerImageVersionId":30762,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.5"}},"nbformat":4,"nbformat_minor":4}
